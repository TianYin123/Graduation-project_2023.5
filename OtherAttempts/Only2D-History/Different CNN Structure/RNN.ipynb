{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed08faf-5f76-4fa9-96ed-a562ac164454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:ASD, train:99, valid:12, test:13\n",
      "Class:TD, train:99, valid:12, test:13\n"
     ]
    }
   ],
   "source": [
    "#数据集划分\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "\n",
    "def makedir(new_dir):\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    random.seed(1)\n",
    "\n",
    "    dataset_dir = os.path.join(\"ABIDE\", \"ABIDE_db\")\n",
    "    split_dir = os.path.join(\"ABIDE\", \"ABIDE_split\")\n",
    "    train_dir = os.path.join(split_dir, \"train\")\n",
    "    valid_dir = os.path.join(split_dir, \"valid\")\n",
    "    test_dir = os.path.join(split_dir, \"test\")\n",
    "\n",
    "    train_pct = 0.8\n",
    "    valid_pct = 0.1\n",
    "    test_pct = 0.1\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        for sub_dir in dirs:\n",
    "\n",
    "            txts = os.listdir(os.path.join(root, sub_dir))\n",
    "            txts = list(filter(lambda x: x.endswith('.txt'), txts))\n",
    "            random.shuffle(txts)\n",
    "            txt_count = len(txts)\n",
    "\n",
    "            train_point = int(txt_count * train_pct)\n",
    "            valid_point = int(txt_count * (train_pct + valid_pct))\n",
    "\n",
    "            for i in range(txt_count):\n",
    "                if i < train_point:\n",
    "                    out_dir = os.path.join(train_dir, sub_dir)\n",
    "                elif i < valid_point:\n",
    "                    out_dir = os.path.join(valid_dir, sub_dir)\n",
    "                else:\n",
    "                    out_dir = os.path.join(test_dir, sub_dir)\n",
    "\n",
    "                makedir(out_dir)\n",
    "\n",
    "                target_path = os.path.join(out_dir, txts[i])\n",
    "                src_path = os.path.join(dataset_dir, sub_dir, txts[i])\n",
    "\n",
    "                shutil.copy(src_path, target_path)\n",
    "\n",
    "            print('Class:{}, train:{}, valid:{}, test:{}'.format(sub_dir, train_point, valid_point-train_point,\n",
    "                                                                 txt_count-valid_point))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389c060-088f-4376-9304-f281c8d75fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "random.seed(1)\n",
    "rmb_label = {\"ASD\": 0, \"TD\": 1}      # 设置标签\n",
    "\n",
    "class ABIDEtxtDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        ABIDE_db的Dataset\n",
    "        :param data_dir: str, 数据集所在路径\n",
    "        :param transform: torch.transform，数据预处理\n",
    "        \"\"\"\n",
    "        self.label_name = {\"ASD\": 0, \"TD\": 1}\n",
    "        self.data_info = self.get_txt_info(data_dir)  # data_info存储所有txt路径和标签，在DataLoader中通过index读取样本\n",
    "        self.transform = transform\n",
    "        print('Number of samples:', len(self.data_info))\n",
    "        print('Sample info:', self.data_info[0])\n",
    "    def __getitem__(self, index):\n",
    "        path_txt, label = self.data_info[index]\n",
    "        txt_data = np.loadtxt(path_txt)\n",
    "        txt = torch.tensor(txt_data)     \n",
    "\n",
    "        if self.transform is not None:\n",
    "            txt = self.transform(txt)   # 在这里做transform，转为tensor等等\n",
    "\n",
    "        return txt, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_txt_info(data_dir):\n",
    "        data_info = list()\n",
    "        for root, dirs, _ in os.walk(data_dir):\n",
    "            # 遍历类别\n",
    "            for sub_dir in dirs:\n",
    "                txt_names = os.listdir(os.path.join(root, sub_dir))\n",
    "                txt_names = list(filter(lambda x: x.endswith('.txt'), txt_names))\n",
    "\n",
    "                # 遍历txt\n",
    "                for i in range(len(txt_names)):\n",
    "                    txt_name = txt_names[i]\n",
    "                    path_txt = os.path.join(root, sub_dir, txt_name)\n",
    "                    label = rmb_label[sub_dir]\n",
    "                    data_info.append((path_txt, int(label)))\n",
    "\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bdf64e9-1593-4274-96fd-16e3dbce445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏层状态\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # 前向传播 RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.dropout(out)\n",
    "        # 将输出结果展平\n",
    "        out = out[:, -1, :]\n",
    "        # 全连接层\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adeef45-304d-4914-8c71-dcb4d0480230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集地址over\n",
      "定义超参数over\n",
      "Number of samples: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m定义超参数over\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 实例化数据集和数据加载器\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mABIDEtxtDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m ABIDEtxtDataset(valid_dir ,transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m, in \u001b[0;36mABIDEtxtDataset.__init__\u001b[0;34m(self, data_dir, transform)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of samples:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_info))\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample info:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#数据集地址\n",
    "split_dir = os.path.join(\"ABIDE\", \"ABIDE_split\")  \n",
    "train_dir = os.path.join(split_dir, \"train\")\n",
    "valid_dir = os.path.join(split_dir, \"valid\")\n",
    "test_dir  = os.path.join(split_dir, \"test\") \n",
    "print(\"数据集地址over\")\n",
    "\n",
    "# 定义超参数\n",
    "input_size = 111   # 根据具体数据集修改\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "print(\"定义超参数over\")\n",
    "\n",
    "# 实例化数据集和数据加载器\n",
    "train_dataset = ABIDEtxtDataset(train_dir ,transform=None)\n",
    "valid_dataset = ABIDEtxtDataset(valid_dir ,transform=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(\"实例化数据集和数据加载器over\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11362fff-41de-4bb6-9c7b-6083861a7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型、损失函数和优化器\n",
    "model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.float())\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算训练准确率和损失\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_acc += (predicted == label).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, label) in enumerate(valid_loader):\n",
    "            outputs = model(data.float())\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # 计算验证准确率和损失\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            valid_acc += (predicted == label).sum().item()\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    # 计算平均损失和准确率\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    valid_acc /= len(valid_loader.dataset)\n",
    "\n",
    "    # 保存损失和准确率\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    # 打印训练结果\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
    "\n",
    "# 可视化训练结果\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(valid_losses, label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accs, label=\"train acc\")\n",
    "plt.plot(valid_accs, label=\"valid acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5844fa-d4c9-46a3-8a25-e0f8767f2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试数据\n",
    "test_dir  = os.path.join(split_dir, \"test\") \n",
    "test_dataset = ABIDEtxtDataset(test_dir, transform=None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 使用测试数据进行模型的评估\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.double()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Test Accuracy: {:.2f}%'.format(accuracy))\n",
    "\n",
    "# 将模型恢复为训练模式\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c46dca-6e54-4445-b37a-8bb641058ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
